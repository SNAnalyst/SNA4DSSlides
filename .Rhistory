lm(loan_amount ~ ., data = loans[, -fs[7]])
lm(loan_amount ~ ., data = loans[, -fs[8]])
lm(loan_amount ~ ., data = loans[, -fs[9]])
lm(loan_amount ~ ., data = loans[, -fs[10]])
lm(loan_amount ~ ., data = loans[, -fs[11]])
lm(loan_amount ~ ., data = loans[, -fs[12]])
fs
lm(loan_amount ~ application_type, data = loans)
lm(loan_amount ~ ., data = loans[, -40) ########
lm(loan_amount ~ ., data = loans[, -40]) ########
loans[. 40]
loans[, 40]
loans$application_type
is.na(loans$application_type)
any(is.na(loans$application_type))
levels(loans$application_type)
levels(loans$application_type) <- c("individual","joint"  )
any((loans$application_type))
table(loans$application_type)
?relevel
?levels
## drop the now-unused levels, leaving only "primary" and "green"
loans$application_type <- droplevels(loans$application_type)
levels(loans$application_type)
lm(loan_amount ~ ., data = loans)
levels(loans$application_type)
table(loans$application_type)
lm(loan_amount ~ ., data = loans)
lm(loan_amount ~ ., data = loans[, -fs[6]]) ########
eod::get_quota()
source("D:/Dropbox/R/R Projects/eod/converteer_deelfiles_naar_parquet.R", echo = TRUE)
sna4tutti::open_sna4tutti_tutorials()
converteer_deelfiles_naar_parquet("US", 2004, "US_1m_2004")
maak_hive("US", 2004)
eod::get_quota()
install.packages("tm", dependencies = TRUE)         # Text mining
install.packages("SnowballC", dependencies = TRUE)  # Text stemming
install.packages("wordcloud", dependencies = TRUE)  # Word cloud plotting
install.packages("RColorBrewer", dependencies = TRUE) # Nice color palettes
# Read in the text
text <- base::readLines("C:\hulp\vita.txt")
# Read in the text
text <- base::readLines("C:\\hulp\\vita.txt")
# Create a text corpus
corpus <- tm::Corpus(tm::VectorSource(text))
# Clean and process the corpus
corpus <- tm::tm_map(corpus, tm::content_transformer(base::tolower))
corpus <- tm::tm_map(corpus, tm::removeNumbers)
corpus <- tm::tm_map(corpus, tm::removeWords, tm::stopwords("english"))
corpus <- tm::tm_map(corpus, tm::removePunctuation)
corpus <- tm::tm_map(corpus, tm::stripWhitespace)
# Read in the text
text <- base::readLines("C:\\hulp\\vita.txt", encoding = "UTF-8", warn = FALSE)
# Create a text corpus
corpus <- tm::Corpus(tm::VectorSource(text))
# Clean and process the corpus
corpus <- tm::tm_map(corpus, tm::content_transformer(base::tolower))
corpus <- tm::tm_map(corpus, tm::removeNumbers)
corpus <- tm::tm_map(corpus, tm::removeWords, tm::stopwords("english"))
corpus <- tm::tm_map(corpus, tm::removePunctuation)
corpus <- tm::tm_map(corpus, tm::stripWhitespace)
# You can remove extra unwanted characters if needed:
toSpace <- tm::content_transformer(function(x, pattern) base::gsub(pattern, " ", x))
corpus <- tm::tm_map(corpus, toSpace, "/|@|\\|")
# build term matrix
tdm <- tm::TermDocumentMatrix(corpus)
mat <- base::as.matrix(tdm)
freq <- base::sort(base::rowSums(mat), decreasing = TRUE)
df <- base::data.frame(word = base::names(freq), freq = freq)
# draw the work cloud
wordcloud::wordcloud(
words      = df$word,
freq       = df$freq,
min.freq   = 2, # adjust for your needs
max.words  = 100,
random.order = FALSE,
rot.per    = 0.35,
colors     = RColorBrewer::brewer.pal(8, "Dark2")
)
# Create a text corpus
corpus <- tm::Corpus(tm::VectorSource(text))
# Clean and process the corpus
corpus <- tm::tm_map(corpus, tm::content_transformer(base::tolower))
corpus <- tm::tm_map(corpus, tm::removeNumbers)
corpus <- tm::tm_map(corpus, tm::removeWords, tm::stopwords("english"))
custom_stopwords <- c("van", "course", "member", "faculty", "arena", "title", "joint", "year", "school", "march", "dept")
corpus <- tm::tm_map(corpus, tm::removeWords, custom_stopwords)
corpus <- tm::tm_map(corpus, tm::removePunctuation)
corpus <- tm::tm_map(corpus, tm::stripWhitespace)
corpus <- tm::tm_map(
corpus,
tm::content_transformer(SnowballC::wordStem),
language = "english"
)
# You can remove extra unwanted characters if needed:
toSpace <- tm::content_transformer(function(x, pattern) base::gsub(pattern, " ", x))
corpus <- tm::tm_map(corpus, toSpace, "/|@|\\|")
# build term matrix
tdm <- tm::TermDocumentMatrix(corpus)
mat <- base::as.matrix(tdm)
freq <- base::sort(base::rowSums(mat), decreasing = TRUE)
df <- base::data.frame(word = base::names(freq), freq = freq)
# draw the work cloud
wordcloud::wordcloud(
words      = df$word,
freq       = df$freq,
min.freq   = 2, # adjust for your needs
max.words  = 100,
random.order = FALSE,
rot.per    = 0.35,
colors     = RColorBrewer::brewer.pal(8, "Dark2")
)
# draw the work cloud
wordcloud::wordcloud(
words      = df$word,
freq       = df$freq,
min.freq   = 2, # adjust for your needs
max.words  = 100,
random.order = FALSE,
rot.per    = 0.35,
colors     = RColorBrewer::brewer.pal(8, "Dark2")
)
# Read in the text
text <- base::readLines("C:\\hulp\\vita.txt", encoding = "UTF-8", warn = FALSE)
# Create a text corpus
corpus <- tm::Corpus(tm::VectorSource(text))
multiword_terms <- c("social capital", "relatoinal event", "social network")
synonyms <- list(
college = "university",
dept = "department",
postdoc = "research",
"associate professor" = "professor",
tiu = "tilburg",
tsb = "tilburg",
course = "courses",
processes = "process",
networking = "network",
organizational = "organization",
organizations = "organization",
"new product" = "innovation",
bayesian = "bayes"
)
custom_stopwords <- c("van", "course", "member", "faculty", "arena", "title", "joint",
"year", "school", "march", "sept", "dept", "leenders", "goedee", "approach",
"voor", "defense", "major", "several", "york", "review", "containing",
"department", "rthj", "jml", "kratzer", "engelen", "current", "studies"
)
# Clean and process the corpus
corpus <- tm::tm_map(corpus, tm::content_transformer(base::tolower))
corpus <- tm::tm_map(corpus, tm::removeNumbers)
corpus <- tm::tm_map(corpus, tm::removeWords, tm::stopwords("english"))
corpus <- tm::tm_map(corpus, tm::removeWords, custom_stopwords)
corpus <- tm::tm_map(corpus, tm::removePunctuation)
corpus <- tm::tm_map(corpus, tm::stripWhitespace)
corpus <- tm::tm_map(
corpus,
tm::content_transformer(SnowballC::wordStem),
language = "english"
)
# You can remove extra unwanted characters if needed:
toSpace <- tm::content_transformer(function(x, pattern) base::gsub(pattern, " ", x))
corpus <- tm::tm_map(corpus, toSpace, "/|@|\\|")
# build term matrix
tdm <- tm::TermDocumentMatrix(corpus)
mat <- base::as.matrix(tdm)
freq <- base::sort(base::rowSums(mat), decreasing = TRUE)
df <- base::data.frame(word = base::names(freq), freq = freq)
# draw the work cloud
wordcloud::wordcloud(
words      = df$word,
freq       = df$freq,
min.freq   = 2, # adjust for your needs
max.words  = 100,
random.order = FALSE,
rot.per    = 0.35,
colors     = RColorBrewer::brewer.pal(8, "Dark2")
)
# Read in the text
text <- base::readLines("C:\\hulp\\vita.txt", encoding = "UTF-8", warn = FALSE)
# Create a text corpus
corpus <- tm::Corpus(tm::VectorSource(text))
multiword_terms <- c("social capital", "relatoinal event", "social network",
"human capital")
synonyms <- list(
college = "university",
dept = "department",
postdoc = "research",
"associate professor" = "professor",
tiu = "tilburg",
tsb = "tilburg",
courses = "course",
processes = "process",
networking = "network",
organizational = "organization",
organizations = "organization",
"new product" = "innovation",
"product development" = "innovation",
bayesian = "bayes"
)
custom_stopwords <- c("van", "course", "member", "faculty", "arena", "title", "joint",
"year", "school", "march", "sept", "dept", "leenders", "goedee", "approach",
"voor", "defense", "major", "several", "york", "review", "containing",
"department", "rthj", "jml", "kratzer", "engelen", "current", "studies",
"mulder", "gabbay"
)
keep_together <- function(text, phrases) {
for (phrase in phrases) {
replacement <- base::gsub(" ", "_", phrase)
pattern <- base::gsub(" ", "\\\\s+", phrase)    # Pattern for whitespace between words
text <- base::gsub(pattern, replacement, text, ignore.case = TRUE)
}
text
}
replace_synonyms <- function(text, syn_list) {
for (i in base::seq_along(syn_list)) {
pattern <- base::paste0("\\b", base::names(syn_list)[i], "\\b")
text <- base::gsub(pattern, syn_list[[i]], text, ignore.case = TRUE)
}
text
}
# 3. Read and clean text
text <- base::readLines(file_path, encoding = "UTF-8", warn = FALSE)
text <- base::iconv(text, from = "", to = "ASCII", sub = "")
# 4. Keep multiword terms together
text <- keep_together(text, multiword_terms)
# 5. Create corpus
corpus <- tm::Corpus(tm::VectorSource(text))
# 6. Main cleaning pipeline
corpus <- tm::tm_map(corpus, tm::content_transformer(base::tolower))
synonyms <- list(
college = "university",
dept = "department",
postdoc = "research",
"associate professor" = "professor",
tiu = "tilburg",
tsb = "tilburg",
courses = "course",
processes = "process",
networking = "network",
organizational = "organization",
organizations = "organization",
"new product" = "innovation",
"product development" = "innovation",
bayesian = "bayes",
REM = "relational event"
)
custom_stopwords <- c("van", "course", "member", "faculty", "arena", "title", "joint",
"year", "school", "march", "sept", "dept", "leenders", "goedee", "approach",
"voor", "defense", "major", "several", "york", "review", "containing",
"department", "rthj", "jml", "kratzer", "engelen", "current", "studies",
"mulder", "gabbay"
)
keep_together <- function(text, phrases) {
for (phrase in phrases) {
replacement <- base::gsub(" ", "_", phrase)
pattern <- base::gsub(" ", "\\\\s+", phrase)    # Pattern for whitespace between words
text <- base::gsub(pattern, replacement, text, ignore.case = TRUE)
}
text
}
replace_synonyms <- function(text, syn_list) {
for (i in base::seq_along(syn_list)) {
pattern <- base::paste0("\\b", base::names(syn_list)[i], "\\b")
text <- base::gsub(pattern, syn_list[[i]], text, ignore.case = TRUE)
}
text
}
# Read in the text
text <- base::readLines("C:\\hulp\\vita.txt", encoding = "UTF-8", warn = FALSE)
text <- base::iconv(text, from = "", to = "ASCII", sub = "")
# 4. Keep multiword terms together
text <- keep_together(text, multiword_terms)
# 5. Create corpus
corpus <- tm::Corpus(tm::VectorSource(text))
# 6. Main cleaning pipeline
corpus <- tm::tm_map(corpus, tm::content_transformer(base::tolower))
corpus <- tm::tm_map(corpus, tm::removeNumbers)
corpus <- tm::tm_map(corpus, tm::removeWords, tm::stopwords("english"))
corpus <- tm::tm_map(corpus, tm::removeWords, custom_stopwords)
corpus <- tm::tm_map(corpus, tm::removePunctuation)
corpus <- tm::tm_map(corpus, tm::stripWhitespace)
# 7. Synonym replacement (after punctuation removed so words don't run together)
corpus <- tm::tm_map(
corpus,
tm::content_transformer(function(x) replace_synonyms(x, synonyms))
)
# 8. Stemming (merges word forms)
corpus <- tm::tm_map(
corpus,
tm::content_transformer(SnowballC::wordStem),
language = "english"
)
# 9. Build term-document matrix and frequencies
tdm <- tm::TermDocumentMatrix(corpus)
mat <- base::as.matrix(tdm)
freq <- base::sort(base::rowSums(mat), decreasing = TRUE)
df <- base::data.frame(word = base::names(freq), freq = freq)
# 10. Draw the word cloud
wordcloud::wordcloud(
words      = df$word,
freq       = df$freq,
min.freq   = 2,
max.words  = 100,
random.order = FALSE,
rot.per    = 0.35,
colors     = RColorBrewer::brewer.pal(8, "Dark2")
)
custom_stopwords <- c("van", "course", "member", "faculty", "arena", "title", "joint",
"year", "school", "march", "sept", "dept", "leenders", "goedee", "approach",
"voor", "defense", "major", "several", "york", "review", "containing",
"department", "rthj", "jml", "kratzer", "engelen", "current", "studies",
"mulder", "gabbay", "journal", "zucca", "univers", "vos"
)
keep_together <- function(text, phrases) {
for (phrase in phrases) {
replacement <- base::gsub(" ", "_", phrase)
pattern <- base::gsub(" ", "\\\\s+", phrase)    # Pattern for whitespace between words
text <- base::gsub(pattern, replacement, text, ignore.case = TRUE)
}
text
}
replace_synonyms <- function(text, syn_list) {
for (i in base::seq_along(syn_list)) {
pattern <- base::paste0("\\b", base::names(syn_list)[i], "\\b")
text <- base::gsub(pattern, syn_list[[i]], text, ignore.case = TRUE)
}
text
}
# Read in the text
text <- base::readLines("C:\\hulp\\vita.txt", encoding = "UTF-8", warn = FALSE)
text <- base::iconv(text, from = "", to = "ASCII", sub = "")
# 4. Keep multiword terms together
text <- keep_together(text, multiword_terms)
# 5. Create corpus
corpus <- tm::Corpus(tm::VectorSource(text))
# 6. Main cleaning pipeline
corpus <- tm::tm_map(corpus, tm::content_transformer(base::tolower))
corpus <- tm::tm_map(corpus, tm::removeNumbers)
corpus <- tm::tm_map(corpus, tm::removeWords, tm::stopwords("english"))
corpus <- tm::tm_map(corpus, tm::removeWords, custom_stopwords)
corpus <- tm::tm_map(corpus, tm::removePunctuation)
corpus <- tm::tm_map(corpus, tm::stripWhitespace)
# 7. Synonym replacement (after punctuation removed so words don't run together)
corpus <- tm::tm_map(
corpus,
tm::content_transformer(function(x) replace_synonyms(x, synonyms))
)
# 8. Stemming (merges word forms)
corpus <- tm::tm_map(
corpus,
tm::content_transformer(SnowballC::wordStem),
language = "english"
)
# 9. Build term-document matrix and frequencies
tdm <- tm::TermDocumentMatrix(corpus)
mat <- base::as.matrix(tdm)
freq <- base::sort(base::rowSums(mat), decreasing = TRUE)
df <- base::data.frame(word = base::names(freq), freq = freq)
# 10. Draw the word cloud
wordcloud::wordcloud(
words      = df$word,
freq       = df$freq,
min.freq   = 2,
max.words  = 100,
random.order = FALSE,
rot.per    = 0.35,
colors     = RColorBrewer::brewer.pal(8, "Dark2")
)
source("~/.active-rstudio-document", echo = TRUE)
`r chevron_right()`  (on Wednesdays) are online (Zoom)
renderthis::to_pdf("D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_01a.Rmd", "D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_01a.pdf", complex_slides = FALSE, partial_slides = FALSE)
#' As a useful convention for the title of the deck in index.html (this is
#' given in \code{lecture_description}) is:
#'
#' Week 02 -- Lab: Intro to R in SNA
#'
#' @param rmd_file the full path to the Rmd file that needs to be parsed (typically a path on your own computer)
#' @param lecture_description the title for the slide deck
#'
#' @return code to be copied as is into index.rmd
#' @keywords internal
make_index_code <- function(rmd_file, lecture_description = "Week 02 -- Lab: Intro to R in SNA") {
slidedeckFile <- strsplit(rmd_file, "/") |>
{\(x) x[[1]]}() |>
{\(x) x[length(x)]}() |>
stringr::str_remove(".Rmd") |>
stringr::str_remove(".rmd")
txt <- readLines(rmd_file)
name_locs <- grepl('^name:', txt) |> which()
description_locs <- grepl('^description:', txt) |> which()
output <- matrix(ncol = 2, nrow = length(name_locs))
output[, 1] <- txt[name_locs] |>
stringr::str_remove('name:') |>
stringr::str_trim()
for (nn in 1:nrow(output)) {
descr <- description_locs %in% c(name_locs[nn] - 1, name_locs[nn] + 1) |> which()
if (length(descr) != 0) {  # there is a matching description for the name
if (length(descr) == 1) {
output[nn, 2] <- txt[description_locs[descr]] |>
stringr::str_remove('description:') |>
stringr::str_remove_all('"') |>
stringr::str_trim()
} else if (length(descr) > 1) {
stop("name '", output[nn, 1], " 'has multiple descriptions next to it, make sure to only have 1")
}
code_matrix <- data.frame(matrix("", ncol = 1, nrow = 12 + nrow(output)))
code_matrix[1, ] <- paste0("<!-- ", lecture_description, " -->")
code_matrix[2, ] <- "```{r, include = FALSE}"
code_matrix[3, ] <- "overview <- matrix(c("
final_name <- 3 + nrow(output)
code_matrix[4:final_name, ] <- glue::glue('"{name}", "{description}",', name = output[, 1], description = output[, 2])
# remove the last comma for the last one
code_matrix[final_name, 1] <- stringr::str_remove(code_matrix[(3 + nrow(output)), 1], ",$")
code_matrix[final_name + 1, ] <- ("), byrow = TRUE, ncol = 2) |> ")
code_matrix[final_name + 2, ] <- "as.data.frame() |> "
code_matrix[final_name + 3, ] <- 'setNames(c("name", "description"))'
code_matrix[final_name + 5, ] <- glue::glue('slidedeckFile <- "{slidedeckFile}"')
code_matrix[final_name + 6, ] <- glue::glue('lectureName <- "{lecture_description}"')
code_matrix[final_name + 7, ] <- "```"
code_matrix[final_name + 9, ] <- "`r make_overview(overview, path, slidedeckFile, lectureName)`"
colnames(code_matrix) <- NULL
print(code_matrix, right = FALSE, quote = FALSE, row.names = FALSE)
}
make_index_code("D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_01a.Rmd", "Week 01 -- Lecture: Intro to the course")
source("D:/Dropbox/R/R Projects/eod/converteer_deelfiles_naar_parquet.R", echo = TRUE)
make_index_code("D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.Rmd", "Week 01 -- Lecture: Network measures")
renderthis::to_pdf("D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html", "D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.pdf", complex_slides = TRUE, partial_slides = FALSE, delay = 5)
setwd("D:\\Dropbox\\R\\eigen_packages\\=git\\SNA4DS_slides")
setwd("D:\\Dropbox\\R\\eigen_packages\\=git\\SNA4DSSlides")
# lecture MEASURES
slides <- c(
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#1",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#4",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#5",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#6",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#11",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#15",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#16",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#18",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#20",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#21",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#23",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#24",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#27",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#28",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#30",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#31",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#32",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#33",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#34",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#35",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#36",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#38",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#39",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#40",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#41",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#45"
)
len <- length(slides)
namen <- paste0("slide", stringr::str_pad(1:len, width = 2, "left", 0), ".png")
for (ss in 1:len) {
webshot2::webshot(slides[ss], file = paste0("./webshot2/", namen[ss]), delay = 15,
vwidth = 1920, vheight = 1080, zoom = 2)
}
getwd()
for (ss in 1:len) {
webshot2::webshot(slides[ss], file = paste0("./webshot2/", namen[ss]), delay = 15,
vwidth = 1920, vheight = 1080, zoom = 2)
}
setwd("D:\\Dropbox\\R\\eigen_packages\\=git\\SNA4DSSlides")
# lecture MEASURES
slides <- c(
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#1",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#4",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#5",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#6",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#11",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#15",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#16",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#18",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#20",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#21",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#23",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#24",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#27",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#28",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#30",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#31",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#32",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#33",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#34",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#35",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#36",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#38",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#39",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#40",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#41",
"file:///D:/Dropbox/R/eigen_packages/=git/SNA4DSSlides/Lecture_Measures.html#45"
)
len <- length(slides)
namen <- paste0("slide", stringr::str_pad(1:len, width = 2, "left", 0), ".png")
for (ss in 1:len) {
webshot2::webshot(slides[ss], file = paste0("./webshot2/", namen[ss]), delay = 15,
vwidth = 1920, vheight = 1080, zoom = 2)
}
# List your PNG files in order
png_files <- list.files(path = "./webshot2", pattern = "\\.png$", full.names = TRUE) |> sort()
png_files
# Read all PNG images as magick images
imgs <- magick::image_read(png_files)
# Write to PDF (each PNG on one PDF page)
magick::image_write(imgs, path = "Week01_Measures_lecture.pdf", format = "pdf")
